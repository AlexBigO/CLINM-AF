# <font color="darkmagenta">**CLINM Analysis Framework (CLINM-AF)**</font>

A set of python modules and scripts to:
- analyse experimental data from CLINM measurements
- produce simulated data (only GATE10 for the moment)

# <font color="blue">**Choices**</font>

We decide to work with the following environment:
- Python scripts (python + PyROOT)
- the scripts shall remain generic and work with YAML configuration files as input
- we develop utils that will be put in the dedicated folder to get lighter "main scripts"


# <font color="blue">**Architecture**</font>

*IDEA*: reformat all the input data so that each config (each data taking) gets associated to a repository

- `Utils`: a directory with some utilities used in several scripts across the framework
- `DataImport`: a directory with the necessary tools to retrieve data from remote server
- `Fitter`: (not implemented yet) a directory with the tools used to fit energy distributions


# <font color="blue">**Data production via STIVI [WORK IN PROGRESS]**</font>

We generate raw data reconstruction via STIVI with flatTree option enabled:

<put Christian's script>

# <font color="blue">**Import data from remote server**</font>

We want to retrieve data from a server (e.g. `sbgui11`). These data can either be:
- data that already that underwent STIVI reconstruction (with flatTree option enabled!)
- data from the Wave Catcher (so `.bin` files) that would then require some STIVI treatment

To import data, do:
```
cd DataImport
python3 import_data_from_server.py config_import_data.yml
```

*Notes*:
- One can decide to print the copy and rename command lines, or run them directly
- The file(s) or directory than should be copied from remote to local disc need to be specified in the config file
- One can copy:
    - a single file
    - a list of files
    - a single directory
    - a list of directories

    and rename them if desired

- It is also possible to create a SSH Control Master and add it directly to the `~/.ssh/` config file


*Caveat*: this script only works when already connected to IPHC server.

# <font color="blue">**Handle data from STIVI output**</font>

For each `.root` file we want to analyse:
- we compute kinetic energies before Plastic 1 and before Plastic 2, and store them in new branches;
- we create new `.root` files containing only the original `TTree` and some QA plots (the 2D plots).

To do so, we run:

```python3 read_from_stivi.py config_read_from_stivi.yml```

where:
- `read_from_stivi.py` is a generic python script that should not be modified;
- `config_read_from_stivi.yml` is a configuration file that must me modified to adapt the input and output to the use-case.

This script produces a directory (one for each input file, i.e. one per Run) with a name associated to the Run details contained in the input `.root` file. This directory is where the output is stored. This directory will then be the location of the subsequent analysis steps.

*Note*: it was chosen to work with a directory per Run because of the many files that might be produced during the next steps of the analysis.

# <font color="blue">**Calibration of the detectors [WORK IN PROGRESS]**</font>

Three scintillation detectors are used in this set:
- a "thin" plastic (2 mm)
- a "thicker" plastic (4mm)
- an organic CeBr3 detector used as a calorimeter

We measure (notably) the amplitude of the signal generated by these detectors during an event. Yet we need to link this amplitude (or the charge, defined as the amplitude intergrated over the measurement time window) to the energy deposited by the detected particle. Hence, the need of a calibration that shall provide a dictionary allowing for the translation of measured amplitude into deposited energy.

One needs to perform this calibration in two steps.

## Step 1: get amplitude from real data

## Step 2: get deposited energy from simulation

We use Gate to perform this simulation


# <font color="blue">**Fitting tool [WORK IN PROGRESS]**</font>

*Note*: We decide to use a ROOT-based approach as ROOT is more well known by the team members (than some *flarefly* package for instance). Besides, this is a versatile option as one could work with ROOT or pyROOT, making it "internship-compatible".

*Note*: I would like to do some unbinned fits, hence RooFit :)

*Note*: the fitter should run on a whole tree, because it would need the information of the whole phase space to improve fitting parameters...

*Idea*: the base structure of flarefly could be re-used.

This tool would be used to perform fit in 2D plan (Delta E - E) and extract nsigma values for each point in this plan (i.e. do PID).

It would be nice to go deep in the fit theory to extract some p-value info, nll info and so on...

## <font color="darkgreen">**Tests**</font>

I decided to perform tests on data from Run 06 located in `/Users/abigot/CLINM/Data/CNAO/spt2025/rootfiles/Run_06_Data_9_13_2025_Binary_config2_carbon120MeVu_FlatTree` folder.

The goal is to write a script that will allow to scan the data in order to check the distributions per energy bin.

## <font color="darkgreen">**Components**</font>

- data handler
- fitter
- store results
- quality assurance

## <font color="darkgreen">**Data handler**</font>

Handle input (.root) files with TTrees and perform the discretisation of space along x axis.

It will store these new distributions into a dedicated `.root`file.

To run this process, do:
```
python3 projector.py config_projector.yml
```

*Question*: how to choose the energy binning ? We decide to do it by eye at first but we are developing a more clever and automatised way to do it.

### Energy pre-binning


So, we first try a dummy scan of the bins and then adapt it by adding a constraint on the minimal number of points that should be inside a bin. **NEED TO FIND THIS NUMBER: a percentage of the total amount of data maybe ?**

### Extrema of the distributions

The point is that for the fits, we'll need to have some insight on the initial values of the parameters (e.g. mean, standard deviation, fitting range) and we also need to be sure that there is enough data inside am energy bin to be fitted. So, once the pre-binning is done, we perform a scan of the distribution in every bin in order to retrieve estimations of local minima and maxima:
- the maxima would be associated to a peak hence will feed the initial value of the fit functions means;
- the minima would give information on the fitting range (between two minima for a given peak).

We run this scan and store the informations on the same file as the `TTree`s with the energy distributions.


## <font color="darkgreen">**The fitter itself**</font>

### Initialisation

We use `RooFit` to fit data in each energy bin. As mentioned above, we shall use the information retrieved on extrema to give a starting point to the fit parameters and also constrain the fitting ranges (one peak for each value of $Z$).

### The fit

We perform the fit and store all relevant information (related to the fit result) into a new file.

### ?

It must be able to perform fits in energy bins for a whole 2D plot, and then do an iterative work to improve the global fit with Bethe-Bloch.